"source": [
    "#specify parameters here:\n",
    "episodes=1000\n",
    "\n",
    "#Randomly initialize critic,actor,target critic, target actor network  and replay buffer   \n",
    "# agent  = DDPG(enviroment, users_num, items_num, num_actions, STATE_SIZE, output_dim)  # output_dim là output của State_emebedding, để 1445 vì đầu vào của actor.evaluate_actor là (1445,400)\n",
    "exploration_noise = OUNoise(num_actions)\n",
    "counter=0\n",
    "reward_per_episode = 0    \n",
    "total_reward=0\n",
    "#saving reward:\n",
    "reward_st = np.array([0])\n",
    "    \n",
    "environment = StimulateEnv(users_dict, STATE_SIZE)\n",
    "agent  = DDPG(environment, users_num, items_num, num_actions, STATE_SIZE, output_dim)  # output_dim là output của State_emebedding, để 1445 vì đầu vào của actor.evaluate_actor là (1445,400)\n",
    "for i in range(0, episodes):\n",
    "    print(\"==== Starting episode no:\",i,\"====\",\"\\n\")\n",
    "    STATE_SIZE = 20 ## + embedding_dim vs STATE_SIZE in DDPG, num_history in ENVIRONMENT\n",
    "    user_id, watched_videos, done = environment.reset()\n",
    "    users_history_lens = STATE_SIZE\n",
    "    reward_per_episode = 0\n",
    "    steps = len(users_dict[user_id][users_history_lens:])\n",
    "    \n",
    "    for t in range(1, steps-1):\n",
    "        environment = StimulateEnv(users_dict, STATE_SIZE, fix_user_id=user_id)\n",
    "        x = environment.old_watched\n",
    "        ## change shape to fit evaluate_actor \n",
    "        state_value = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "        state_value = tf.expand_dims(state_value, axis=0)\n",
    "        action = agent.evaluate_actor(np.reshape(state_value,[1, num_actions]))\n",
    "        noise = exploration_noise.noise()\n",
    "        action = action[0] + noise #Select action according to current policy and exploration noise\n",
    "        # print(\"Action at step\", t ,\" :\",action,\"\\n\")\n",
    "        recommended_item = agent.recommend_item(action, all_items, environment.old_watched, top_k= 20)\n",
    "\n",
    "        next_items_ids_embs, reward, done, _= environment.step(recommended_item)\n",
    "\n",
    "        state_value_next = tf.convert_to_tensor(next_items_ids_embs, dtype=tf.float32)\n",
    "        state_value_next = tf.expand_dims(state_value_next, axis=0)\n",
    "        state_value_next = np.reshape(state_value_next,[1, num_actions])\n",
    "\n",
    "        # print(environment.user_id)\n",
    "        # print(\"history to embs:\", x)\n",
    "        # print(\"env items:\", environment.items)\n",
    "        # print(\"suggested items:\", recommended_item)\n",
    "        # print(\"actually watch\", environment.newest_watched_video)\n",
    "        # print(\"old watch\", environment.old_watched)\n",
    "        # print(\"next items to embs:\", next_items_ids_embs)\n",
    "        # print(users_dict[user_id])\n",
    "        # print(reward)\n",
    "        # print(\"_\"*69)\n",
    "        \n",
    "        STATE_SIZE+=1  ## giả lập truyền liên tục vào Enviroment các video mới cho lần kế tiếp\n",
    "        ## Xử lý  phần newest_watch_video và phần state_size và embedding_dim và action_núms\n",
    "        \n",
    "        #add s_t,s_t+1,action,reward to experience memory\n",
    "        agent.add_experience(state_value, state_value_next, action, reward, done)\n",
    "        #train critic and actor network\n",
    "        if counter > 20: \n",
    "            agent.train()\n",
    "        reward_per_episode+=reward[0]\n",
    "        counter+=1\n",
    "        #check if episode ends:\n",
    "        if (done or (t == steps-1) or t == 20):\n",
    "            print('EPISODE: ',i,' Steps: ',t,' Total Reward: ',reward_per_episode)\n",
    "            print(\"Printing reward to file\")\n",
    "            exploration_noise.reset() #reinitializing random noise for action exploration\n",
    "            reward_st = np.append(reward_st,reward_per_episode)\n",
    "            np.savetxt('/home/tuannm84/Desktop/longbien/Project/DDPG/episode_reward_movielens.txt',reward_st, newline=\"\\n\")\n",
    "            print('\\n\\n')\n",
    "            break\n",
    "total_reward+=reward_per_episode            \n",
    "print(\"Average reward per episode {}\".format(total_reward / episodes)) \n",
    "print(\"Total reward per episode {}\".format(total_reward)) "
   ]
  },
